import { describe, it, expect } from 'vitest';
import { tokenize } from './lexer';
import { TokenType } from './types';

describe('Lexer - Default Delimiters', () => {
  describe('Basic Tokenization', () => {
    it('should tokenize plain text', () => {
      const tokens = tokenize('Hello World');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].type).toBe(TokenType.TEXT);
      expect(tokens[0].content).toBe('Hello World');
    });

    it('should tokenize empty string', () => {
      const tokens = tokenize('');
      expect(tokens).toHaveLength(0);
    });

    it('should tokenize single expression', () => {
      const tokens = tokenize('{{ name }}');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].type).toBe(TokenType.EXPRESSION);
      expect(tokens[0].content).toBe('{{ name }}');
    });

    it('should tokenize single statement', () => {
      const tokens = tokenize('{% if true %}');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].type).toBe(TokenType.STATEMENT);
      expect(tokens[0].content).toBe('{% if true %}');
    });

    it('should tokenize single comment', () => {
      const tokens = tokenize('{# comment #}');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].type).toBe(TokenType.COMMENT);
      expect(tokens[0].content).toBe('{# comment #}');
    });

    it('should tokenize text with expression', () => {
      const tokens = tokenize('Hello {{ name }}!');
      expect(tokens).toHaveLength(3);
      expect(tokens[0].type).toBe(TokenType.TEXT);
      expect(tokens[0].content).toBe('Hello ');
      expect(tokens[1].type).toBe(TokenType.EXPRESSION);
      expect(tokens[1].content).toBe('{{ name }}');
      expect(tokens[2].type).toBe(TokenType.TEXT);
      expect(tokens[2].content).toBe('!');
    });

    it('should tokenize text with statement', () => {
      const tokens = tokenize('Start {% if x %} End');
      expect(tokens).toHaveLength(3);
      expect(tokens[0].content).toBe('Start ');
      expect(tokens[1].content).toBe('{% if x %}');
      expect(tokens[2].content).toBe(' End');
    });

    it('should tokenize text with comment', () => {
      const tokens = tokenize('Before {# note #} After');
      expect(tokens).toHaveLength(3);
      expect(tokens[0].content).toBe('Before ');
      expect(tokens[1].content).toBe('{# note #}');
      expect(tokens[2].content).toBe(' After');
    });
  });

  describe('Mixed Token Types', () => {
    it('should tokenize expression and statement', () => {
      const tokens = tokenize('{{ x }}{% if y %}');
      expect(tokens).toHaveLength(2);
      expect(tokens[0].type).toBe(TokenType.EXPRESSION);
      expect(tokens[1].type).toBe(TokenType.STATEMENT);
    });

    it('should tokenize statement and expression', () => {
      const tokens = tokenize('{% for i %}{{ i }}');
      expect(tokens).toHaveLength(2);
      expect(tokens[0].type).toBe(TokenType.STATEMENT);
      expect(tokens[1].type).toBe(TokenType.EXPRESSION);
    });

    it('should tokenize all token types', () => {
      const tokens = tokenize('Text {{ expr }} {% stmt %} {# comment #}');
      expect(tokens).toHaveLength(6);
      expect(tokens[0].type).toBe(TokenType.TEXT);
      expect(tokens[1].type).toBe(TokenType.EXPRESSION);
      expect(tokens[2].type).toBe(TokenType.TEXT);
      expect(tokens[3].type).toBe(TokenType.STATEMENT);
      expect(tokens[4].type).toBe(TokenType.TEXT);
      expect(tokens[5].type).toBe(TokenType.COMMENT);
    });

    it('should handle adjacent expressions', () => {
      const tokens = tokenize('{{ a }}{{ b }}{{ c }}');
      expect(tokens).toHaveLength(3);
      expect(tokens[0].content).toBe('{{ a }}');
      expect(tokens[1].content).toBe('{{ b }}');
      expect(tokens[2].content).toBe('{{ c }}');
    });

    it('should handle adjacent statements', () => {
      const tokens = tokenize('{% if a %}{% endif %}{% for x %}');
      expect(tokens).toHaveLength(3);
      expect(tokens[0].content).toBe('{% if a %}');
      expect(tokens[1].content).toBe('{% endif %}');
      expect(tokens[2].content).toBe('{% for x %}');
    });

    it('should handle adjacent comments', () => {
      const tokens = tokenize('{# one #}{# two #}{# three #}');
      expect(tokens).toHaveLength(3);
      expect(tokens[0].content).toBe('{# one #}');
      expect(tokens[1].content).toBe('{# two #}');
      expect(tokens[2].content).toBe('{# three #}');
    });
  });

  describe('Complex Templates', () => {
    it('should tokenize if-else block', () => {
      const template = '{% if user %}Hello {{ user.name }}{% else %}Guest{% endif %}';
      const tokens = tokenize(template);
      expect(tokens).toHaveLength(6);
      expect(tokens[0].type).toBe(TokenType.STATEMENT);
      expect(tokens[1].type).toBe(TokenType.TEXT);
      expect(tokens[2].type).toBe(TokenType.EXPRESSION);
      expect(tokens[3].type).toBe(TokenType.STATEMENT);
      expect(tokens[4].type).toBe(TokenType.TEXT);
      expect(tokens[5].type).toBe(TokenType.STATEMENT);
    });

    it('should tokenize for loop', () => {
      const template = '{% for item in items %}{{ item }}{% endfor %}';
      const tokens = tokenize(template);
      expect(tokens).toHaveLength(3);
      expect(tokens[0].content).toBe('{% for item in items %}');
      expect(tokens[1].content).toBe('{{ item }}');
      expect(tokens[2].content).toBe('{% endfor %}');
    });

    it('should tokenize nested structures', () => {
      const template = '{% if x %}{% for y %}{{ y }}{% endfor %}{% endif %}';
      const tokens = tokenize(template);
      expect(tokens).toHaveLength(5);
    });
  });

  describe('Whitespace Handling', () => {
    it('should preserve whitespace in text', () => {
      const tokens = tokenize('  Hello  World  ');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('  Hello  World  ');
    });

    it('should preserve whitespace around expressions', () => {
      const tokens = tokenize('  {{ x }}  ');
      expect(tokens).toHaveLength(3);
      expect(tokens[0].content).toBe('  ');
      expect(tokens[2].content).toBe('  ');
    });

    it('should handle tabs', () => {
      const tokens = tokenize('\t{{ x }}\t');
      expect(tokens).toHaveLength(3);
      expect(tokens[0].content).toBe('\t');
      expect(tokens[2].content).toBe('\t');
    });

    it('should handle newlines in text', () => {
      const tokens = tokenize('Line 1\nLine 2\nLine 3');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('Line 1\nLine 2\nLine 3');
    });

    it('should handle windows newlines', () => {
      const tokens = tokenize('Line 1\r\nLine 2');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('Line 1\r\nLine 2');
    });

    it('should handle mixed newlines', () => {
      const tokens = tokenize('Unix\nWindows\r\nMac\r');
      expect(tokens).toHaveLength(1);
    });
  });

  describe('Special Characters', () => {
    it('should handle single braces in text', () => {
      const tokens = tokenize('{ hello }');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('{ hello }');
    });

    it('should handle partial delimiters in text', () => {
      const tokens = tokenize('{ % } { { } }');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('{ % } { { } }');
    });

    it('should handle special regex characters', () => {
      const tokens = tokenize('$^*+?.()|[]{}\\');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('$^*+?.()|[]{}\\');
    });

    it('should handle unicode characters', () => {
      const tokens = tokenize('Hello ä¸–ç•Œ ðŸŒ');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('Hello ä¸–ç•Œ ðŸŒ');
    });

    it('should handle unicode in expressions', () => {
      const tokens = tokenize('{{ ä¸–ç•Œ }}');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('{{ ä¸–ç•Œ }}');
    });
  });

  describe('Expression Content', () => {
    it('should tokenize simple variable', () => {
      const tokens = tokenize('{{ x }}');
      expect(tokens[0].content).toBe('{{ x }}');
    });

    it('should tokenize dot notation', () => {
      const tokens = tokenize('{{ user.name.first }}');
      expect(tokens[0].content).toBe('{{ user.name.first }}');
    });

    it('should tokenize array access', () => {
      const tokens = tokenize('{{ items[0] }}');
      expect(tokens[0].content).toBe('{{ items[0] }}');
    });

    it('should tokenize function calls', () => {
      const tokens = tokenize('{{ format(date) }}');
      expect(tokens[0].content).toBe('{{ format(date) }}');
    });

    it('should tokenize operators', () => {
      const tokens = tokenize('{{ a + b * c }}');
      expect(tokens[0].content).toBe('{{ a + b * c }}');
    });

    it('should tokenize string literals', () => {
      const tokens = tokenize('{{ "hello world" }}');
      expect(tokens[0].content).toBe('{{ "hello world" }}');
    });

    it('should tokenize number literals', () => {
      const tokens = tokenize('{{ 42 }}');
      expect(tokens[0].content).toBe('{{ 42 }}');
    });

    it('should tokenize boolean literals', () => {
      const tokens = tokenize('{{ true }}');
      expect(tokens[0].content).toBe('{{ true }}');
    });

    it('should tokenize null', () => {
      const tokens = tokenize('{{ null }}');
      expect(tokens[0].content).toBe('{{ null }}');
    });

    it('should handle complex expressions', () => {
      const tokens = tokenize('{{ user.items[idx].name || "default" }}');
      expect(tokens[0].content).toBe('{{ user.items[idx].name || "default" }}');
    });
  });

  describe('Statement Content', () => {
    it('should tokenize if statement', () => {
      const tokens = tokenize('{% if condition %}');
      expect(tokens[0].content).toBe('{% if condition %}');
    });

    it('should tokenize else statement', () => {
      const tokens = tokenize('{% else %}');
      expect(tokens[0].content).toBe('{% else %}');
    });

    it('should tokenize elif statement', () => {
      const tokens = tokenize('{% elif other %}');
      expect(tokens[0].content).toBe('{% elif other %}');
    });

    it('should tokenize endif statement', () => {
      const tokens = tokenize('{% endif %}');
      expect(tokens[0].content).toBe('{% endif %}');
    });

    it('should tokenize for statement', () => {
      const tokens = tokenize('{% for item in items %}');
      expect(tokens[0].content).toBe('{% for item in items %}');
    });

    it('should tokenize endfor statement', () => {
      const tokens = tokenize('{% endfor %}');
      expect(tokens[0].content).toBe('{% endfor %}');
    });

    it('should tokenize while statement', () => {
      const tokens = tokenize('{% while condition %}');
      expect(tokens[0].content).toBe('{% while condition %}');
    });

    it('should tokenize set statement', () => {
      const tokens = tokenize('{% set x = 10 %}');
      expect(tokens[0].content).toBe('{% set x = 10 %}');
    });

    it('should tokenize import statement', () => {
      const tokens = tokenize('{% import "module" %}');
      expect(tokens[0].content).toBe('{% import "module" %}');
    });

    it('should handle complex conditions', () => {
      const tokens = tokenize('{% if a > 5 and b < 10 or c == null %}');
      expect(tokens[0].content).toBe('{% if a > 5 and b < 10 or c == null %}');
    });
  });

  describe('Comment Content', () => {
    it('should tokenize simple comment', () => {
      const tokens = tokenize('{# note #}');
      expect(tokens[0].content).toBe('{# note #}');
    });

    it('should handle comments with text', () => {
      const tokens = tokenize('{# This is a comment #}');
      expect(tokens[0].content).toBe('{# This is a comment #}');
    });

    it('should handle comments with special chars', () => {
      const tokens = tokenize('{# TODO: fix @#$%^&* #}');
      expect(tokens[0].content).toBe('{# TODO: fix @#$%^&* #}');
    });

    it('should handle multi-line comments', () => {
      const tokens = tokenize('{# line 1\nline 2\nline 3 #}');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].type).toBe(TokenType.COMMENT);
    });

    it('should handle empty comments', () => {
      const tokens = tokenize('{##}');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('{##}');
    });
  });
});

describe('Lexer - Position Tracking', () => {
  describe('Single Line Positions', () => {
    it('should track position for single text token', () => {
      const tokens = tokenize('Hello');
      expect(tokens[0].start).toEqual({ line: 1, column: 0 });
      expect(tokens[0].end).toEqual({ line: 1, column: 5 });
    });

    it('should track position for single expression', () => {
      const tokens = tokenize('{{ x }}');
      expect(tokens[0].start).toEqual({ line: 1, column: 0 });
      expect(tokens[0].end).toEqual({ line: 1, column: 7 });
    });

    it('should track position for single statement', () => {
      const tokens = tokenize('{% if x %}');
      expect(tokens[0].start).toEqual({ line: 1, column: 0 });
      expect(tokens[0].end).toEqual({ line: 1, column: 10 });
    });

    it('should track position for single comment', () => {
      const tokens = tokenize('{# note #}');
      expect(tokens[0].start).toEqual({ line: 1, column: 0 });
      expect(tokens[0].end).toEqual({ line: 1, column: 10 });
    });

    it('should track positions for multiple tokens', () => {
      const tokens = tokenize('A{{ x }}B');
      expect(tokens[0].start).toEqual({ line: 1, column: 0 });
      expect(tokens[0].end).toEqual({ line: 1, column: 1 });
      expect(tokens[1].start).toEqual({ line: 1, column: 1 });
      expect(tokens[1].end).toEqual({ line: 1, column: 8 });
      expect(tokens[2].start).toEqual({ line: 1, column: 8 });
      expect(tokens[2].end).toEqual({ line: 1, column: 9 });
    });
  });

  describe('Multi-Line Positions', () => {
    it('should track position across lines for text', () => {
      const tokens = tokenize('Line 1\nLine 2');
      expect(tokens[0].start).toEqual({ line: 1, column: 0 });
      expect(tokens[0].end).toEqual({ line: 2, column: 6 });
    });

    it('should track position for tokens on different lines', () => {
      const tokens = tokenize('Line 1\n{{ x }}\nLine 3');
      expect(tokens[0].start.line).toBe(1);
      expect(tokens[1].start.line).toBe(2);
      expect(tokens[2].start.line).toBe(2);
    });

    it('should track position for multi-line expression', () => {
      const tokens = tokenize('{{\n  x\n}}');
      expect(tokens[0].start).toEqual({ line: 1, column: 0 });
      expect(tokens[0].end.line).toBe(3);
    });

    it('should track position for multi-line statement', () => {
      const tokens = tokenize('{%\n  if x\n%}');
      expect(tokens[0].start).toEqual({ line: 1, column: 0 });
      expect(tokens[0].end.line).toBe(3);
    });

    it('should track position for multi-line comment', () => {
      const tokens = tokenize('{#\n  comment\n#}');
      expect(tokens[0].start).toEqual({ line: 1, column: 0 });
      expect(tokens[0].end.line).toBe(3);
    });
  });

  describe('Column Tracking', () => {
    it('should track columns correctly with spaces', () => {
      const tokens = tokenize('  {{ x }}  ');
      expect(tokens[0].start.column).toBe(0);
      expect(tokens[0].end.column).toBe(2);
      expect(tokens[1].start.column).toBe(2);
      expect(tokens[1].end.column).toBe(9);
      expect(tokens[2].start.column).toBe(9);
      expect(tokens[2].end.column).toBe(11);
    });

    it('should track columns with tabs', () => {
      const tokens = tokenize('\t{{ x }}\t');
      expect(tokens[0].start.column).toBe(0);
      expect(tokens[1].start.column).toBe(1);
      expect(tokens[2].start.column).toBe(8);
    });
  });
});

describe('Lexer - Custom Delimiters', () => {
  describe('Single Custom Delimiter', () => {
    it('should use custom expression delimiters', () => {
      const tokens = tokenize('[[ x ]]', {
        delimiters: {
          expression_start: '[[',
          expression_end: ']]',
        },
      });
      expect(tokens).toHaveLength(1);
      expect(tokens[0].type).toBe(TokenType.EXPRESSION);
      expect(tokens[0].content).toBe('[[ x ]]');
    });

    it('should use custom statement delimiters', () => {
      const tokens = tokenize('<% if x %>', {
        delimiters: {
          statement_start: '<%',
          statement_end: '%>',
        },
      });
      expect(tokens).toHaveLength(1);
      expect(tokens[0].type).toBe(TokenType.STATEMENT);
      expect(tokens[0].content).toBe('<% if x %>');
    });

    it('should use custom comment delimiters', () => {
      const tokens = tokenize('/* note */', {
        delimiters: {
          comment_start: '/*',
          comment_end: '*/',
        },
      });
      expect(tokens).toHaveLength(1);
      expect(tokens[0].type).toBe(TokenType.COMMENT);
      expect(tokens[0].content).toBe('/* note */');
    });
  });

  describe('Multiple Custom Delimiters', () => {
    it('should use all custom delimiters', () => {
      const tokens = tokenize('[[ x ]] <% if y %> /* note */', {
        delimiters: {
          expression_start: '[[',
          expression_end: ']]',
          statement_start: '<%',
          statement_end: '%>',
          comment_start: '/*',
          comment_end: '*/',
        },
      });
      expect(tokens).toHaveLength(5);
      expect(tokens[0].type).toBe(TokenType.EXPRESSION);
      expect(tokens[2].type).toBe(TokenType.STATEMENT);
      expect(tokens[4].type).toBe(TokenType.COMMENT);
    });

    it('should mix custom and default delimiters', () => {
      const tokens = tokenize('[[ x ]] {% if y %}', {
        delimiters: {
          expression_start: '[[',
          expression_end: ']]',
        },
      });
      expect(tokens).toHaveLength(3);
      expect(tokens[0].content).toBe('[[ x ]]');
      expect(tokens[2].content).toBe('{% if y %}');
    });
  });

  describe('Special Custom Delimiters', () => {
    it('should handle regex special chars in delimiters', () => {
      const tokens = tokenize('${ x }', {
        delimiters: {
          expression_start: '${',
          expression_end: '}',
        },
      });
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('${ x }');
    });

    it('should handle long delimiters', () => {
      const tokens = tokenize('<<<START x END>>>', {
        delimiters: {
          expression_start: '<<<START',
          expression_end: 'END>>>',
        },
      });
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('<<<START x END>>>');
    });

    it('should handle single char delimiters', () => {
      const tokens = tokenize('< x >', {
        delimiters: {
          expression_start: '<',
          expression_end: '>',
        },
      });
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('< x >');
    });

    it('should handle asymmetric delimiters', () => {
      const tokens = tokenize('{{ x }}}', {
        delimiters: {
          expression_start: '{{',
          expression_end: '}}}',
        },
      });
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('{{ x }}}');
    });
  });

  describe('Delimiter Edge Cases', () => {
    it('should handle delimiters with whitespace', () => {
      const tokens = tokenize('< % x % >', {
        delimiters: {
          statement_start: '< %',
          statement_end: '% >',
        },
      });
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('< % x % >');
    });

    it('should treat old delimiters as text with custom delimiters', () => {
      const tokens = tokenize('{{ x }} [[ y ]]', {
        delimiters: {
          expression_start: '[[',
          expression_end: ']]',
        },
      });
      expect(tokens).toHaveLength(2);
      expect(tokens[0].type).toBe(TokenType.TEXT);
      expect(tokens[0].content).toBe('{{ x }} ');
      expect(tokens[1].type).toBe(TokenType.EXPRESSION);
      expect(tokens[1].content).toBe('[[ y ]]');
    });
  });
});

describe('Lexer - Error Handling', () => {
  describe('Unclosed Delimiters', () => {
    it('should error on unclosed expression', () => {
      expect(() => tokenize('{{ x')).toThrow(/unclosed expression/i);
    });

    it('should error on unclosed statement', () => {
      expect(() => tokenize('{% if x')).toThrow(/unclosed statement/i);
    });

    it('should error on unclosed comment', () => {
      expect(() => tokenize('{# note')).toThrow(/unclosed comment/i);
    });

    it('should report position in error for unclosed expression', () => {
      expect(() => tokenize('{{ x')).toThrow(/line 1.*column 0/i);
    });

    it('should report position in error for unclosed statement', () => {
      expect(() => tokenize('{% if')).toThrow(/line 1.*column 0/i);
    });

    it('should report position in error for unclosed comment', () => {
      expect(() => tokenize('{# note')).toThrow(/line 1.*column 0/i);
    });

    it('should error on unclosed expression with text before', () => {
      expect(() => tokenize('Hello {{ x')).toThrow(/unclosed expression/i);
    });

    it('should error on unclosed expression on second line', () => {
      expect(() => tokenize('Line 1\n{{ x')).toThrow(/line 2/i);
    });
  });

  describe('Malformed Delimiters', () => {
    it('should handle partial expression start', () => {
      const tokens = tokenize('{ { x }}');
      // This should be treated as text since delimiter doesn't match
      expect(tokens[0].type).toBe(TokenType.TEXT);
    });

    it('should handle partial statement start', () => {
      const tokens = tokenize('{ % x %}');
      expect(tokens[0].type).toBe(TokenType.TEXT);
    });

    it('should handle partial comment start', () => {
      const tokens = tokenize('{ # x #}');
      expect(tokens[0].type).toBe(TokenType.TEXT);
    });
  });
});

describe('Lexer - Edge Cases', () => {
  describe('Empty Content', () => {
    it('should handle empty expressions', () => {
      const tokens = tokenize('{{}}');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('{{}}');
    });

    it('should handle empty statements', () => {
      const tokens = tokenize('{%%}');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('{%%}');
    });

    it('should handle empty comments', () => {
      const tokens = tokenize('{##}');
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toBe('{##}');
    });
  });

  describe('Delimiter Lookalikes', () => {
    it('should not confuse similar delimiters', () => {
      const tokens = tokenize('{{ x }} {% y %} {# z #}');
      expect(tokens[0].type).toBe(TokenType.EXPRESSION);
      expect(tokens[2].type).toBe(TokenType.STATEMENT);
      expect(tokens[4].type).toBe(TokenType.COMMENT);
    });

    it('should handle nested braces in expressions', () => {
      const tokens = tokenize('{{ {a: 1} }}');
      expect(tokens[0].type).toBe(TokenType.EXPRESSION);
      expect(tokens[0].content).toBe('{{ {a: 1} }}');
    });

    it('should handle nested braces in statements', () => {
      const tokens = tokenize('{% set x = {a: 1} %}');
      expect(tokens[0].type).toBe(TokenType.STATEMENT);
    });
  });

  describe('Long Content', () => {
    it('should handle very long text', () => {
      const longText = 'a'.repeat(10000);
      const tokens = tokenize(longText);
      expect(tokens).toHaveLength(1);
      expect(tokens[0].content).toHaveLength(10000);
    });

    it('should handle many tokens', () => {
      const template = '{{ x }} '.repeat(1000);
      const tokens = tokenize(template);
      expect(tokens.length).toBeGreaterThan(1000);
    });
  });

  describe('Real-World Templates', () => {
    it('should tokenize HTML template', () => {
      const template = '<div>{{ user.name }}</div>';
      const tokens = tokenize(template);
      expect(tokens).toHaveLength(3);
      expect(tokens[0].content).toBe('<div>');
      expect(tokens[1].content).toBe('{{ user.name }}');
      expect(tokens[2].content).toBe('</div>');
    });

    it('should tokenize JSON template', () => {
      const template = '{"name": "{{ name }}", "age": {{ age }}}';
      const tokens = tokenize(template);
      expect(tokens.some((t) => t.type === TokenType.EXPRESSION)).toBe(true);
    });

    it('should tokenize YAML template', () => {
      const template = 'name: {{ name }}\nage: {{ age }}';
      const tokens = tokenize(template);
      expect(tokens.some((t) => t.type === TokenType.EXPRESSION)).toBe(true);
    });

    it('should tokenize Markdown template', () => {
      const template = '# {{ title }}\n\n{{ content }}';
      const tokens = tokenize(template);
      expect(tokens.some((t) => t.type === TokenType.EXPRESSION)).toBe(true);
    });

    it('should tokenize SQL template', () => {
      const template = 'SELECT * FROM users WHERE id = {{ userId }}';
      const tokens = tokenize(template);
      expect(tokens.some((t) => t.type === TokenType.EXPRESSION)).toBe(true);
    });
  });
});

describe('Lexer - Performance', () => {
  it.skip('should tokenize 4KB template in < 1ms', () => {
    // Create a 4KB template
    const chunk = 'Text {{ expr }} {% stmt %} {# comment #}\n';
    const iterations = Math.ceil(4096 / chunk.length);
    const template = chunk.repeat(iterations);

    const start = performance.now();
    tokenize(template);
    const duration = performance.now() - start;

    expect(duration).toBeLessThan(1);
  });

  it('should tokenize 10KB plain text quickly', () => {
    const template = 'a'.repeat(10240);
    const start = performance.now();
    tokenize(template);
    const duration = performance.now() - start;

    // CI runners can be noisier; keep a stricter local target.
    const thresholdMs = process.env.CI ? 5 : 2;
    expect(duration).toBeLessThan(thresholdMs);
  });

  it('should tokenize 100 expressions quickly', () => {
    const template = '{{ x }}'.repeat(100);
    const start = performance.now();
    tokenize(template);
    const duration = performance.now() - start;

    // Increased threshold to 10ms to account for CI environment variability
    expect(duration).toBeLessThan(10);
  });
});

describe('Lexer - Integration Tests', () => {
  it('should tokenize complete if-else template', () => {
    const template = `
{% if user %}
  <div class="user">
    <h1>{{ user.name }}</h1>
    <p>{{ user.email }}</p>
  </div>
{% else %}
  <div class="guest">
    <p>Please log in</p>
  </div>
{% endif %}
    `.trim();

    const tokens = tokenize(template);
    expect(tokens.length).toBe(9);
  });

  it('should tokenize loop with filter', () => {
    const template = `
{% for item in items %}
  {{ item.name }}: {{ item.value }}
{% endfor %}
    `.trim();

    const tokens = tokenize(template);
    expect(tokens.some((t) => t.content.includes('for'))).toBe(true);
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION).length).toBe(2);
  });

  it('should tokenize nested loops', () => {
    const template = `
{% for category in categories %}
  <h2>{{ category.name }}</h2>
  {% for item in category.items %}
    <li>{{ item }}</li>
  {% endfor %}
{% endfor %}
    `.trim();

    const tokens = tokenize(template);
    expect(tokens.filter((t) => t.content.includes('for')).length).toBe(4);
  });

  it('should tokenize template with comments', () => {
    const template = `
{# User profile section #}
<div>
  {# Display user name #}
  {{ user.name }}
  {# Display user bio #}
  {{ user.bio }}
</div>
    `.trim();

    const tokens = tokenize(template);
    expect(tokens.filter((t) => t.type === TokenType.COMMENT).length).toBe(3);
  });
});

describe('Lexer - Unicode and Special Characters', () => {
  it('should tokenize emoji in text', () => {
    const tokens = tokenize('Hello ðŸŒ {{ name }}');
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(1);
  });

  it('should tokenize Chinese characters in text', () => {
    const tokens = tokenize('ä½ å¥½ {{ name }}');
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(1);
  });

  it('should tokenize Arabic text', () => {
    const tokens = tokenize('Ù…Ø±Ø­Ø¨Ø§ {{ name }}');
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(1);
  });

  it('should tokenize mixed scripts', () => {
    const tokens = tokenize('Hello à¦¹à§à¦¯à¦¾à¦²à§‹ {{ name }} ä½ å¥½');
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(1);
  });

  it('should tokenize special UTF-8 characters', () => {
    const tokens = tokenize('CafÃ© {{ name }} naÃ¯ve');
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(1);
  });

  it('should handle control characters', () => {
    const tokens = tokenize('Text\u0000With\u0001Control');
    expect(tokens.length).toBeGreaterThan(0);
  });

  it('should handle tabs in text', () => {
    const tokens = tokenize('Text\t{{ x }}\t');
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(1);
  });

  it('should handle mixed whitespace', () => {
    const tokens = tokenize('Text  \n  \r\n  \t  {{ x }}');
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(1);
  });
});

describe('Lexer - Multiline Templates', () => {
  it('should tokenize expression across lines', () => {
    const tokens = tokenize('{{ \n  name \n }}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.EXPRESSION);
  });

  it('should tokenize statement with multiline content', () => {
    const tokens = tokenize('{% if\n  condition\n%}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.STATEMENT);
  });

  it('should track line numbers correctly', () => {
    const template = 'Line1\n{{ x }}\nLine3';
    const tokens = tokenize(template);
    expect(tokens.length).toBeGreaterThan(0);
    // Verify we have multiple tokens at different lines
    const expressionTokens = tokens.filter((t) => t.type === TokenType.EXPRESSION);
    expect(expressionTokens.length).toBeGreaterThan(0);
  });

  it('should handle expressions on same line', () => {
    const tokens = tokenize('{{ a }} and {{ b }} and {{ c }}');
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(3);
  });

  it('should handle multiple newlines', () => {
    const tokens = tokenize('Text\n\n\n{{ x }}\n\n');
    expect(tokens.length).toBeGreaterThanOrEqual(3);
  });

  it('should handle Windows line endings', () => {
    const tokens = tokenize('Line1\r\n{{ x }}\r\nLine3');
    expect(tokens.length).toBeGreaterThanOrEqual(3);
  });

  it('should handle Mac line endings', () => {
    const tokens = tokenize('Line1\r{{ x }}\rLine3');
    expect(tokens.length).toBeGreaterThanOrEqual(3);
  });

  it('should handle mixed line endings', () => {
    const tokens = tokenize('Line1\r\n{{ x }}\nLine3\rEnd');
    expect(tokens.length).toBeGreaterThanOrEqual(3);
  });
});

describe('Lexer - Position Tracking', () => {
  it('should track column position correctly', () => {
    const tokens = tokenize('12345{{ x }}');
    expect(tokens[0].start.column).toBeDefined();
    expect(tokens[1].start.column).toBeDefined();
  });

  it('should track position after newline', () => {
    const tokens = tokenize('12345\n{{ x }}');
    expect(tokens.length).toBeGreaterThanOrEqual(2);
    // Check that we have proper position information
    const exprTokens = tokens.filter((t) => t.type === TokenType.EXPRESSION);
    expect(exprTokens.length).toBeGreaterThan(0);
  });

  it('should track position with tabs', () => {
    const tokens = tokenize('\t\t{{ x }}');
    expect(tokens[0].start).toBeDefined();
  });

  it('should provide accurate position for nested expressions', () => {
    const tokens = tokenize('{{ a }}{{ b }}{{ c }}');
    const exprTokens = tokens.filter((t) => t.type === TokenType.EXPRESSION);
    expect(exprTokens.length).toBe(3);
    expect(exprTokens[0].start).toBeDefined();
    expect(exprTokens[1].start).toBeDefined();
    expect(exprTokens[2].start).toBeDefined();
  });

  it('should track end position of tokens', () => {
    const tokens = tokenize('{{ name }}');
    expect(tokens[0].start).toBeDefined();
    expect(tokens[0].end).toBeDefined();
    expect(tokens[0].start.line).toBe(1);
  });
});

describe('Lexer - HTML/XML Edge Cases', () => {
  it('should tokenize HTML with attributes', () => {
    const template = '<div class="container" id="{{ id }}">Content</div>';
    const tokens = tokenize(template);
    expect(tokens.some((t) => t.type === TokenType.EXPRESSION)).toBe(true);
  });

  it('should handle HTML comments', () => {
    const template = '<!-- Comment --> {{ x }}';
    const tokens = tokenize(template);
    expect(tokens.some((t) => t.type === TokenType.EXPRESSION)).toBe(true);
  });

  it('should handle self-closing tags', () => {
    const template = '<img src="{{ url }}" />';
    const tokens = tokenize(template);
    expect(tokens.some((t) => t.type === TokenType.EXPRESSION)).toBe(true);
  });

  it('should handle CDATA sections', () => {
    const template = '<![CDATA[raw {{ content }}]]>';
    const tokens = tokenize(template);
    // CDATA sections are treated as text, may be split by tokenizer
    expect(tokens.length).toBeGreaterThan(0);
  });

  it('should handle DOCTYPE', () => {
    const template = '<!DOCTYPE html>\n{{ x }}';
    const tokens = tokenize(template);
    expect(tokens.some((t) => t.type === TokenType.EXPRESSION)).toBe(true);
  });

  it('should handle XML namespaces', () => {
    const template = '<root xmlns:custom="http://example.com">{{ x }}</root>';
    const tokens = tokenize(template);
    expect(tokens.some((t) => t.type === TokenType.EXPRESSION)).toBe(true);
  });
});

describe('Lexer - JSON/YAML/Markdown', () => {
  it('should tokenize JSON object', () => {
    const template = '{"key": "{{ value }}", "nested": {"field": {{ count }}}}';
    const tokens = tokenize(template);
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(2);
  });

  it('should tokenize JSON array', () => {
    const template = '[{{ item1 }}, {{ item2 }}, {{ item3 }}]';
    const tokens = tokenize(template);
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(3);
  });

  it('should tokenize YAML mapping', () => {
    const template = 'key1: {{ value1 }}\nkey2: {{ value2 }}';
    const tokens = tokenize(template);
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(2);
  });

  it('should tokenize YAML list', () => {
    const template = '- {{ item1 }}\n- {{ item2 }}\n- {{ item3 }}';
    const tokens = tokenize(template);
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(3);
  });

  it('should tokenize Markdown heading', () => {
    const template = '# {{ title }}\n\n## {{ subtitle }}';
    const tokens = tokenize(template);
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(2);
  });

  it('should tokenize Markdown list', () => {
    const template = '- Item {{ one }}\n- Item {{ two }}';
    const tokens = tokenize(template);
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(2);
  });

  it('should tokenize Markdown code block', () => {
    const template = '```\n{{ code }}\n```';
    const tokens = tokenize(template);
    expect(tokens.some((t) => t.type === TokenType.EXPRESSION)).toBe(true);
  });
});

describe('Lexer - Boundary Conditions', () => {
  it('should handle expression at start of template', () => {
    const tokens = tokenize('{{ first }} other');
    expect(tokens[0].type).toBe(TokenType.EXPRESSION);
  });

  it('should handle expression at end of template', () => {
    const tokens = tokenize('some {{ last }}');
    expect(tokens[tokens.length - 1].type).toBe(TokenType.EXPRESSION);
  });

  it('should handle only expression', () => {
    const tokens = tokenize('{{ only }}');
    expect(tokens).toHaveLength(1);
  });

  it('should handle alternating expressions and text', () => {
    const tokens = tokenize('a{{ b }}c{{ d }}e{{ f }}');
    // Tokenizes as: text('a'), expr, text('c'), expr, text('e'), expr
    expect(tokens).toHaveLength(6);
  });

  it('should handle zero-width expressions', () => {
    const tokens = tokenize('{{ }}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.EXPRESSION);
  });

  it('should handle very deeply nested object path', () => {
    const expression = '{{ a.b.c.d.e.f.g.h.i.j.k.l.m.n.o.p.q.r.s.t }}';
    const tokens = tokenize(expression);
    expect(tokens).toHaveLength(1);
  });
});

describe('Lexer - Escape Sequences', () => {
  it('should handle escaped braces in text', () => {
    const tokens = tokenize('Text \\{{ not expression }}');
    // Tokenizes as: text('Text \\'), then expr('{{ not expression }}')
    expect(tokens).toHaveLength(2);
  });

  it('should handle backslash at end of text', () => {
    const tokens = tokenize('Text\\');
    expect(tokens).toHaveLength(1);
  });

  it('should handle double backslash', () => {
    const tokens = tokenize('Text\\\\{{ x }}');
    // Tokenizes as: text with double backslash, then expression
    expect(tokens.length).toBeGreaterThanOrEqual(2);
  });

  it('should handle escaped newline', () => {
    const tokens = tokenize('Text\\\nMore');
    // Should tokenize as text across newline
    expect(tokens.length).toBeGreaterThanOrEqual(1);
  });

  it('should handle various escape patterns', () => {
    const tokens = tokenize('\\n\\r\\t{{ x }}');
    // Should have at least text tokens and expression token
    expect(tokens.length).toBeGreaterThanOrEqual(2);
  });
});

describe('Lexer - Comment Variations', () => {
  it('should handle comment with special chars', () => {
    const tokens = tokenize('{# @!$%^&*() #}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.COMMENT);
  });

  it('should handle comment with braces', () => {
    const tokens = tokenize('{# Content with { and } inside #}');
    expect(tokens).toHaveLength(1);
  });

  it('should handle adjacent comments', () => {
    const tokens = tokenize('{# one #}{# two #}{# three #}');
    expect(tokens.filter((t) => t.type === TokenType.COMMENT)).toHaveLength(3);
  });

  it('should handle comment with expression-like content', () => {
    const tokens = tokenize('{# This looks like {{ expression }} but is not #}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.COMMENT);
  });

  it('should handle comment with statement-like content', () => {
    const tokens = tokenize('{# This is {% like a statement %} but just a comment #}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.COMMENT);
  });

  it('should handle multiline comment', () => {
    const tokens = tokenize('{# Line 1\nLine 2\nLine 3 #}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.COMMENT);
  });
});

describe('Lexer - Complex Real-World Scenarios', () => {
  it('should tokenize email template', () => {
    const template = `
Hello {{ user.name }},

Your account: {{ account | upper }}
Balance: {{ balance | currency }}

{% if notifications %}
  Notifications: {{ notifications.count }}
{% endif %}
    `.trim();
    const tokens = tokenize(template);
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION).length).toBeGreaterThan(0);
  });

  it('should tokenize CSV with templates', () => {
    const template = '"{{ id }}","{{ name }}","{{ email }}"';
    const tokens = tokenize(template);
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(3);
  });

  it('should tokenize SQL query', () => {
    const template = `
SELECT * FROM users
WHERE {{ condition }}
AND status = '{{ status }}'
ORDER BY {{ sort_field }} {{ sort_dir }}
    `.trim();
    const tokens = tokenize(template);
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION).length).toBeGreaterThan(0);
  });

  it('should tokenize shell script', () => {
    const template = '#!/bin/bash\necho "{{ message }}"\n{{ command }}';
    const tokens = tokenize(template);
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(2);
  });

  it('should tokenize configuration file', () => {
    const template = `settings:
  debug: {{ debug }}
  user: "{{ username }}"
  timeout: {{ timeout_ms }}`;
    const tokens = tokenize(template);
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(3);
  });
});

describe('Lexer - Filter Chain Patterns', () => {
  it('should tokenize filter with single pipe', () => {
    const tokens = tokenize('{{ name | upper }}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.EXPRESSION);
  });

  it('should tokenize filter with multiple pipes', () => {
    const tokens = tokenize('{{ name | upper | trim }}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].content).toContain('|');
  });

  it('should tokenize filter with arguments', () => {
    const tokens = tokenize('{{ text | replace("a", "b") }}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.EXPRESSION);
  });

  it('should tokenize complex filter chain', () => {
    const tokens = tokenize('{{ user.name | upper | trim | slice(0, 5) }}');
    expect(tokens).toHaveLength(1);
  });
});

describe('Lexer - Statement Block Patterns', () => {
  it('should tokenize if statement', () => {
    const tokens = tokenize('{% if condition %}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.STATEMENT);
  });

  it('should tokenize elseif statement', () => {
    const tokens = tokenize('{% elseif condition %}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.STATEMENT);
  });

  it('should tokenize else statement', () => {
    const tokens = tokenize('{% else %}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.STATEMENT);
  });

  it('should tokenize endif', () => {
    const tokens = tokenize('{% endif %}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.STATEMENT);
  });

  it('should tokenize for loop', () => {
    const tokens = tokenize('{% for item in items %}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.STATEMENT);
  });

  it('should tokenize endfor', () => {
    const tokens = tokenize('{% endfor %}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.STATEMENT);
  });

  it('should tokenize set statement', () => {
    const tokens = tokenize('{% set var = value %}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.STATEMENT);
  });

  it('should tokenize macro definition', () => {
    const tokens = tokenize('{% macro name() %}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.STATEMENT);
  });

  it('should tokenize endmacro', () => {
    const tokens = tokenize('{% endmacro %}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.STATEMENT);
  });

  it('should tokenize block definition', () => {
    const tokens = tokenize('{% block content %}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.STATEMENT);
  });

  it('should tokenize endblock', () => {
    const tokens = tokenize('{% endblock %}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.STATEMENT);
  });
});

describe('Lexer - Complex Expression Patterns', () => {
  it('should tokenize array access', () => {
    const tokens = tokenize('{{ items[0] }}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.EXPRESSION);
  });

  it('should tokenize array access with variable', () => {
    const tokens = tokenize('{{ items[index] }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize nested array access', () => {
    const tokens = tokenize('{{ matrix[0][1] }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize object literal', () => {
    const tokens = tokenize('{{ {a: 1, b: 2} }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize array literal', () => {
    const tokens = tokenize('{{ [1, 2, 3] }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize string literal with escapes', () => {
    const tokens = tokenize('{{ "string with \\"quotes\\"" }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize single quoted string', () => {
    const tokens = tokenize("{{ 'single quoted' }}");
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize template literal', () => {
    const tokens = tokenize('{{ `template ${var} string` }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize function call', () => {
    const tokens = tokenize('{{ func(arg1, arg2) }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize method call', () => {
    const tokens = tokenize('{{ obj.method(arg) }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize chained method calls', () => {
    const tokens = tokenize('{{ obj.method1().method2() }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize ternary operator', () => {
    const tokens = tokenize('{{ condition ? true_val : false_val }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize logical operators', () => {
    const tokens = tokenize('{{ a && b || c }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize comparison operators', () => {
    const tokens = tokenize('{{ a == b && c != d }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize arithmetic operators', () => {
    const tokens = tokenize('{{ a + b - c * d / e }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize modulo operator', () => {
    const tokens = tokenize('{{ a % b }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize exponentiation operator', () => {
    const tokens = tokenize('{{ a ** b }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize unary operators', () => {
    const tokens = tokenize('{{ !condition }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize parenthesized expression', () => {
    const tokens = tokenize('{{ (a + b) * c }}');
    expect(tokens).toHaveLength(1);
  });

  it('should tokenize complex nested expression', () => {
    const tokens = tokenize('{{ (a.b[0] | filter).c.method() }}');
    expect(tokens).toHaveLength(1);
  });
});

describe('Lexer - Edge Cases with Special Sequences', () => {
  it('should handle expression with leading spaces', () => {
    const tokens = tokenize('{{   value   }}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.EXPRESSION);
  });

  it('should handle statement with leading spaces', () => {
    const tokens = tokenize('{%   if x   %}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.STATEMENT);
  });

  it('should handle comment with leading spaces', () => {
    const tokens = tokenize('{#   comment   #}');
    expect(tokens).toHaveLength(1);
    expect(tokens[0].type).toBe(TokenType.COMMENT);
  });

  it('should handle nested delimiters in expression', () => {
    const tokens = tokenize('{{ [1, 2, {a: b}] }}');
    expect(tokens).toHaveLength(1);
  });

  it('should handle string with delimiter look-alikes', () => {
    const tokens = tokenize('{{ "text with {{ in string }}" }}');
    // The tokenizer will split on {{ even inside the string
    // This is expected - users should escape or use different delimiters
    expect(tokens.length).toBeGreaterThan(1);
  });

  it('should handle single statement and expression', () => {
    const template = '{% if x %}{{ y }}{% endif %}';
    const tokens = tokenize(template);
    expect(tokens.filter((t) => t.type === TokenType.STATEMENT)).toHaveLength(2);
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(1);
  });

  it('should handle deeply nested template structure', () => {
    const template = `
      {% if a %}
        {{ b }}
        {% for c in d %}
          {{ e | filter }}
        {% endfor %}
      {% else %}
        {{ f }}
      {% endif %}
    `.trim();
    const tokens = tokenize(template);
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION).length).toBeGreaterThan(0);
    expect(tokens.filter((t) => t.type === TokenType.STATEMENT).length).toBeGreaterThan(0);
  });

  it('should handle expression immediately after statement', () => {
    const tokens = tokenize('{% set x = 1 %}{{ x }}');
    expect(tokens.filter((t) => t.type === TokenType.STATEMENT)).toHaveLength(1);
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(1);
  });

  it('should handle statement immediately after expression', () => {
    const tokens = tokenize('{{ y }}{% if x %}');
    expect(tokens.filter((t) => t.type === TokenType.EXPRESSION)).toHaveLength(1);
    expect(tokens.filter((t) => t.type === TokenType.STATEMENT)).toHaveLength(1);
  });
});
